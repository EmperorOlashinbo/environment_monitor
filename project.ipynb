{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "184d8f1a",
   "metadata": {},
   "source": [
    "# Environmental Sensor with Three Different Algorithms\n",
    "## Anomarlly Detection, Regression and Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5303d2",
   "metadata": {},
   "source": [
    "### Import and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3e99359",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (14, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f329c4",
   "metadata": {},
   "source": [
    "### Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2428c4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (5189, 10)\n",
      "Columns: ['created_at', 'entry_id', 'temperature', 'humidity', 'light_level', 'tilt_staus', 'latitude', 'longitude', 'elevation', 'status']\n",
      "\n",
      "First 5 rows: \n",
      "                   created_at  entry_id  temperature  humidity light_level  \\\n",
      "0  2025-07-12T15:58:07+00:00         1           25        60    13.58297   \n",
      "1  2025-07-12T15:58:27+00:00         2           25        60    15.35546   \n",
      "2  2025-07-12T15:58:58+00:00         3           25        58    12.53559   \n",
      "3  2025-07-12T15:59:18+00:00         4           25        58    15.51659   \n",
      "4  2025-07-12T15:59:39+00:00         5           25        58        16.0   \n",
      "\n",
      "   tilt_staus  latitude  longitude  elevation  status  \n",
      "0    18.99901       NaN        NaN        NaN     NaN  \n",
      "1    18.31388       NaN        NaN        NaN     NaN  \n",
      "2    18.97459       NaN        NaN        NaN     NaN  \n",
      "3    18.60685       NaN        NaN        NaN     NaN  \n",
      "4    18.80369       NaN        NaN        NaN     NaN  \n",
      "\n",
      "Data types:\n",
      " created_at      object\n",
      "entry_id         int64\n",
      "temperature      int64\n",
      "humidity         int64\n",
      "light_level     object\n",
      "tilt_staus     float64\n",
      "latitude       float64\n",
      "longitude      float64\n",
      "elevation      float64\n",
      "status         float64\n",
      "dtype: object\n",
      "\n",
      "Basic stats:\n",
      "           entry_id  temperature     humidity   tilt_staus  latitude  \\\n",
      "count  5189.000000  5189.000000  5189.000000  5189.000000       0.0   \n",
      "mean   2595.000000    26.513972    52.090962     3.191372       NaN   \n",
      "std    1498.079604     2.711602     8.804296     6.872220       NaN   \n",
      "min       1.000000    21.000000    10.000000     0.000000       NaN   \n",
      "25%    1298.000000    25.000000    48.000000     1.000000       NaN   \n",
      "50%    2595.000000    26.000000    54.000000     1.000000       NaN   \n",
      "75%    3892.000000    27.000000    58.000000     1.000000       NaN   \n",
      "max    5189.000000    41.000000    71.000000    97.973600       NaN   \n",
      "\n",
      "       longitude  elevation  status  \n",
      "count        0.0        0.0     0.0  \n",
      "mean         NaN        NaN     NaN  \n",
      "std          NaN        NaN     NaN  \n",
      "min          NaN        NaN     NaN  \n",
      "25%          NaN        NaN     NaN  \n",
      "50%          NaN        NaN     NaN  \n",
      "75%          NaN        NaN     NaN  \n",
      "max          NaN        NaN     NaN  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('feeds.csv')\n",
    "\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(\"\\nFirst 5 rows: \\n\", df.head())\n",
    "print(\"\\nData types:\\n\", df.dtypes)\n",
    "print(\"\\nBasic stats:\\n\", df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901994f4",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "- #### Missing Values, Drop Columns with 100% Missing Values and Datarime Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a47cb94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values:\n",
      "              Missing Count  Percentage\n",
      "created_at               0         0.0\n",
      "entry_id                 0         0.0\n",
      "temperature              0         0.0\n",
      "humidity                 0         0.0\n",
      "light_level              0         0.0\n",
      "tilt_staus               0         0.0\n",
      "latitude              5189       100.0\n",
      "longitude             5189       100.0\n",
      "elevation             5189       100.0\n",
      "status                5189       100.0\n",
      "\n",
      "Dropped: ['latitude', 'longitude', 'elevation', 'status']\n",
      "New Shape: (5189, 6)\n",
      "\n",
      "Date range: 2025-07-12 15:58:07+00:00 to 2025-09-15 12:12:24+00:00\n",
      "Temporal features added.\n"
     ]
    }
   ],
   "source": [
    "# Missing Values\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (missing_values/len(df) * 100)\n",
    "missing_df = pd.DataFrame({'Missing Count': missing_values, 'Percentage': missing_percentage})\n",
    "print(\"Missing values:\\n\", missing_df)\n",
    "\n",
    "# Drop Column with 100% Missing Values\n",
    "columns_to_drop = missing_df[missing_df['Percentage']== 100].index.tolist()\n",
    "df_cleaned = df.drop(columns=columns_to_drop)\n",
    "\n",
    "print(f\"\\nDropped: {columns_to_drop}\")\n",
    "print(f\"New Shape: {df_cleaned.shape}\")\n",
    "\n",
    "# Datetime Features\n",
    "df_cleaned['created_at']= pd.to_datetime(df_cleaned['created_at'])\n",
    "df_cleaned['year']= df_cleaned['created_at'].dt.year\n",
    "df_cleaned['month']= df_cleaned['created_at'].dt.month\n",
    "df_cleaned['day']= df_cleaned['created_at'].dt.day\n",
    "df_cleaned['hour']= df_cleaned['created_at'].dt.hour\n",
    "df_cleaned['minute']= df_cleaned['created_at'].dt.minute\n",
    "df_cleaned['day_of_week']= df_cleaned['created_at'].dt.dayofweek\n",
    "df_cleaned['day_of_year']= df_cleaned['created_at'].dt.dayofyear\n",
    "print(f\"\\nDate range: {df_cleaned['created_at'].min()} to {df_cleaned['created_at'].max()}\")\n",
    "print(\"Temporal features added.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48abd8a1",
   "metadata": {},
   "source": [
    "- #### Clean light_level Column and Dublicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "695a10b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Non-numeric 'light_level': ['()']\n",
      "Filled 331 NaNs in 'light_level' with median value: 24.64\n"
     ]
    }
   ],
   "source": [
    "non_numeric = df_cleaned[pd.to_numeric(df_cleaned['light_level'], errors='coerce').isna()]['light_level'].unique()\n",
    "print(f\"\\nNon-numeric 'light_level': {non_numeric}\")\n",
    "df_cleaned['light_level'] = pd.to_numeric(df_cleaned['light_level'], errors='coerce')\n",
    "nan_count = df_cleaned['light_level'].isna().sum()\n",
    "if nan_count > 0:\n",
    "    median_light = df_cleaned['light_level'].median()\n",
    "    df_cleaned['light_level'].fillna(median_light, inplace=True)\n",
    "    print(f\"Filled {nan_count} NaNs in 'light_level' with median value: {median_light:.2f}\")\n",
    "\n",
    "# Duplicates\n",
    "duplicate_count = df_cleaned.duplicated().sum()\n",
    "if duplicate_count > 0:\n",
    "    df_cleaned = df_cleaned.drop_duplicates()\n",
    "    print(f\"\\nDropped {duplicate_count} duplicate rows.\")\n",
    "\n",
    "# Save cleaned data\n",
    "sensor_cols = ['temperature', 'humidity', 'light_level', 'tilt_status']\n",
    "# df_cleaned.to_csv('feeds_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3b3fcb",
   "metadata": {},
   "source": [
    "## Isolation Forest Class Definations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65259c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IsolationTree:\n",
    "    def __init__(self, height_limit):\n",
    "        self.height_limit = height_limit\n",
    "        self.split_feature = None\n",
    "        self.split_value = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.size = 0\n",
    "        self.is_external = False\n",
    "\n",
    "    def fit(self, X, current_height=0):\n",
    "        self.size = len(X)\n",
    "        if current_height >= self.height_limit or len(X) <= 1:\n",
    "            self.is_external = True\n",
    "            return self\n",
    "        if len(np.unique(X, axis=0)) == 1:\n",
    "            self.is_external = True\n",
    "            return self\n",
    "        n_features = X.shape[1]\n",
    "        self.split_feature = np.random.randint(0, n_features)\n",
    "        feature_values = X[:, self.split_feature]\n",
    "        min_val, max_val = feature_values.min(), feature_values.max()\n",
    "        if min_val == max_val:\n",
    "            self.is_external = True\n",
    "            return self\n",
    "        self.split_value = np.random.uniform(min_val, max_val)\n",
    "        left_mask = feature_values < self.split_value\n",
    "        right_mask = ~left_mask\n",
    "        if left_mask.sum()> 0:\n",
    "            self.left = IsolationTree(self.height_limit)\n",
    "            self.left.fit(X[left_mask], current_height + 1)\n",
    "        if right_mask.sum() > 0:\n",
    "            self.right = IsolationTree(self.height_limit)\n",
    "            self.right.fit(X[right_mask], current_height + 1)\n",
    "        return self\n",
    "    \n",
    "    def path_length(self, x, current_height=0):\n",
    "        if self.is_external:\n",
    "            return current_height + self._c(self.size)\n",
    "        if x[self.split_feature] < self.split_value:\n",
    "            return self.left.path_length(x, current_height + 1) if self.left else current_height + 1\n",
    "        else:\n",
    "            return self.right.path_length(x, current_height + 1) if self.right else current_height + 1\n",
    "        \n",
    "    def _c(self, n):\n",
    "        if n <= 1:\n",
    "            return 0\n",
    "        else:\n",
    "            return 2 * (np.log(n - 1) + 0.5772156649) - (2 * (n - 1) / n)\n",
    "        \n",
    "class IsolationForest:\n",
    "    def __init__(self, n_trees=50, sample_size=256):\n",
    "        self.n_trees = n_trees\n",
    "        self.sample_size = sample_size\n",
    "        self.trees = []\n",
    "        self.height_limit = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        n.samples = len(X)\n",
    "        self.sample_size = min(self.sample_size, n_samples)\n",
    "        self.height_limit = int(np.ceil(np.log2(self.sample_size)))\n",
    "        print(f\"Training IF: {self.n_trees} trees, sample size: {self.sample_size}, height limit: {self.height_limit}\")\n",
    "        for i in range(self.n_trees):\n",
    "            sample_indices = np.random.choice(n_samples, self.sample_size, replace=False)\n",
    "            tree = IsolationTree(self.height_limit)\n",
    "            tree.fit(X[sample_indices])\n",
    "            self.trees.append(tree)\n",
    "            if (i+1) % 10 == 0:\n",
    "                print(f\"  Trees: {i+1}/{self.n_trees} trees\")\n",
    "        return self\n",
    "    \n",
    "    def anomaly_score(self, X):\n",
    "        avg_path = np.mean([tree.path_length(x) for tree in self.trees])\n",
    "        c = 2 * (np.log(self.sample_size - 1) + 0.5772156649) - (2 * (self.sample_size - 1) / self.sample_size)\n",
    "        return 2 ** (-avg_path / c)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.array([self.anomaly_score(x) for x in X])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a694557",
   "metadata": {},
   "source": [
    "## Train and Evaluate Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cd16a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for Isolation Forest\n",
    "sensor_features = ['temperature', 'humidity', 'light_level', 'tilt_status']\n",
    "X_anomaly = df_cleaned[sensor_features].values\n",
    "scaler = StandardScaler()\n",
    "X_anomaly_scaled = scaler.fit_transform(X_anomaly)\n",
    "\n",
    "# Train and predict\n",
    "iforest = IsolationForest()\n",
    "iforest.fit(X_anomaly_scaled)\n",
    "anomaly_scores = iforest.predict(X_anomaly_scaled)\n",
    "df_cleaned['anomaly_score'] = anomaly_scores\n",
    "\n",
    "# Results\n",
    "threshold = 0.6\n",
    "df_cleaned['is_anomaly'] = df_cleaned['anomaly_score'] > threshold\n",
    "n_anomalies = df_cleaned['is_anomaly'].sum()\n",
    "anomaly_percentage = (n_anomalies / len(df_cleaned)) * 100\n",
    "print(f\"Anomalies: {n_anomalies} ({anomaly_percentage:.1f}%) | Threshold: {threshold}\")\n",
    "print(\"\\nScore stats:\\n\", df_cleaned['anomaly_score'].describe())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
